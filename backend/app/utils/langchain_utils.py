"""
LangChain utilities for prompt execution and LLM-based judging.
Provides abstractions for running prompts and evaluating outputs.
"""
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from typing import Dict, Any, Optional
import json
from app.core.config import settings


class PromptExecutor:
    """
    Executes prompts using LangChain.
    Handles template rendering and LLM calls with proper error handling.
    """
    
    def __init__(self, model_name: Optional[str] = None, temperature: Optional[float] = None):
        """
        Initialize prompt executor.
        
        Args:
            model_name: Override default model
            temperature: Override default temperature
        """
        self.model_name = model_name or settings.GENERATION_MODEL
        self.temperature = temperature if temperature is not None else 0.7
        
        # Initialize LLM
        self.llm = ChatOpenAI(
            model=self.model_name,
            temperature=self.temperature,
            api_key=settings.OPENAI_API_KEY,
        )
    
    def execute(
        self,
        template_text: str,
        input_data: Dict[str, Any],
        output_schema: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        Execute a prompt template with given input data.
        
        Args:
            template_text: The prompt template (supports {variable} placeholders)
            input_data: Dictionary of input variables
            output_schema: Optional JSON schema for output parsing
            
        Returns:
            Dictionary containing the LLM output
        """
        try:
            # Create prompt template
            prompt = ChatPromptTemplate.from_template(template_text)
            
            # Format prompt with input data
            formatted_prompt = prompt.format(**input_data)
            
            # Execute prompt
            chain = prompt | self.llm
            response = chain.invoke(input_data)
            
            # Parse response
            if hasattr(response, 'content'):
                content = response.content
            else:
                content = str(response)
            
            # Try to parse as JSON if output schema is provided
            if output_schema:
                try:
                    parsed = json.loads(content)
                    if isinstance(parsed, dict):
                        return parsed
                    else:
                        return {"output": parsed}
                except json.JSONDecodeError:
                    # Not valid JSON, return as text
                    return {"output": content}
            else:
                return {"output": content}
        
        except Exception as e:
            raise ValueError(f"Prompt execution failed: {str(e)}")


class LLMJudge:
    """
    LLM-based judge for evaluating prompt outputs.
    Uses a separate model with low temperature for consistent evaluation.
    Implements blind evaluation (judge doesn't know which prompt generated the output).
    """
    
    def __init__(self, model_name: Optional[str] = None):
        """
        Initialize LLM judge.
        
        Args:
            model_name: Override default judge model
        """
        self.model_name = model_name or settings.JUDGE_MODEL
        
        # Use low temperature for consistent judging
        self.llm = ChatOpenAI(
            model=self.model_name,
            temperature=0.0,  # Deterministic evaluation
            api_key=settings.OPENAI_API_KEY,
        )
    
    def evaluate(
        self,
        input_data: Dict[str, Any],
        actual_output: Dict[str, Any],
        expected_output: Optional[Dict[str, Any]] = None,
        rubric: Optional[str] = None,
        dimensions: Optional[list] = None,
    ) -> Dict[str, Any]:
        """
        Evaluate an output using LLM judge.
        
        Args:
            input_data: Original input to the prompt
            actual_output: Output generated by the prompt
            expected_output: Expected output (if available)
            rubric: Evaluation rubric/criteria
            dimensions: List of dimensions to evaluate
            
        Returns:
            Dictionary with scores and feedback for each dimension
        """
        dimensions = dimensions or ["correctness", "format", "verbosity", "safety"]
        
        # Build evaluation prompt
        evaluation_prompt = self._build_evaluation_prompt(
            input_data, actual_output, expected_output, rubric, dimensions
        )
        
        try:
            # Get judge response
            response = self.llm.invoke(evaluation_prompt)
            judge_output = response.content
            
            # Parse judge response (expecting JSON)
            try:
                # Try to extract JSON from markdown code blocks if present
                if "```json" in judge_output:
                    json_start = judge_output.find("```json") + 7
                    json_end = judge_output.find("```", json_start)
                    judge_output = judge_output[json_start:json_end].strip()
                elif "```" in judge_output:
                    json_start = judge_output.find("```") + 3
                    json_end = judge_output.find("```", json_start)
                    judge_output = judge_output[json_start:json_end].strip()
                
                scores = json.loads(judge_output)
            except json.JSONDecodeError:
                # Fallback: try to extract scores from text
                scores = self._parse_scores_from_text(judge_output, dimensions)
                scores["reasoning"] = judge_output
            
            return {
                "scores": scores,
                "feedback": judge_output,
                "dimensions": dimensions,
            }
        
        except Exception as e:
            raise ValueError(f"LLM judge evaluation failed: {str(e)}")
    
    def _build_evaluation_prompt(
        self,
        input_data: Dict[str, Any],
        actual_output: Dict[str, Any],
        expected_output: Optional[Dict[str, Any]],
        rubric: Optional[str],
        dimensions: list,
    ) -> str:
        """Build the evaluation prompt for the judge"""
        
        prompt = f"""You are an expert evaluator assessing the quality of an AI system's output.

INPUT:
{json.dumps(input_data, indent=2)}

ACTUAL OUTPUT:
{json.dumps(actual_output, indent=2)}
"""
        
        if expected_output:
            prompt += f"""
EXPECTED OUTPUT:
{json.dumps(expected_output, indent=2)}
"""
        
        if rubric:
            prompt += f"""
EVALUATION RUBRIC:
{rubric}
"""
        
        prompt += f"""
Evaluate the output on the following dimensions (score 0.0 to 1.0 for each):
- correctness: Is the output factually correct and addresses the input appropriately?
- format: Does the output match the expected format/structure?
- verbosity: Is the output appropriately detailed (not too brief, not too verbose)?
- safety: Is the output safe, appropriate, and free from harmful content?
- consistency: Is the output internally consistent and coherent?

Respond with a JSON object containing:
{{
  "correctness": <float 0.0-1.0>,
  "format": <float 0.0-1.0>,
  "verbosity": <float 0.0-1.0>,
  "safety": <float 0.0-1.0>,
  "consistency": <float 0.0-1.0>,
  "overall": <float 0.0-1.0>,
  "reasoning": "<brief explanation>"
}}
"""
        
        return prompt
    
    def _parse_scores_from_text(self, text: str, dimensions: list) -> Dict[str, float]:
        """Fallback: try to extract numeric scores from text"""
        scores = {}
        import re
        
        for dim in dimensions + ["overall"]:
            # Look for patterns like "correctness: 0.85" or "correctness score: 0.85"
            pattern = rf"{dim}['\s:]*(\d+\.?\d*)"
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    scores[dim] = float(match.group(1))
                except ValueError:
                    scores[dim] = 0.5  # Default if parsing fails
            else:
                scores[dim] = 0.5  # Default
        
        return scores

